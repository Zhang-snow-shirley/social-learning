{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f80bf910",
   "metadata": {},
   "source": [
    "# AWS Simulation → Reinforcement Learning Pipeline\n",
    "\n",
    "This notebook launches a transient **Dask** cluster on **AWS EC2**, runs the social‐simulation parameter sweep, and then trains the reinforcement‑learning choice model. It mirrors `pipeline.py`, but lets you tweak parameters and inspect intermediate artefacts interactively.\n",
    "\n",
    "**Prerequisites**  \n",
    "‑ An AWS account + IAM permissions to create EC2 instances and S3 objects.  \n",
    "‑ `~/.aws/config` and `~/.aws/credentials` configured on the JupyterLab host.  \n",
    "‑ `pip install dask_cloudprovider s3fs boto3` inside JupyterLab.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4577052d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, contextlib, configparser, pathlib, sys, json, time\n",
    "from platform import python_version\n",
    "\n",
    "import dask\n",
    "from dask_cloudprovider.aws import EC2Cluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "def get_aws_credentials():\n",
    "    \"\"\"Return AWS credentials & region using ~/.aws files.\"\"\"\n",
    "    parser = configparser.RawConfigParser()\n",
    "    parser.read(os.path.expanduser('~/.aws/config'))\n",
    "    cfg_items = parser.items('default') if parser.has_section('default') else []\n",
    "    parser.read(os.path.expanduser('~/.aws/credentials'))\n",
    "    cred_items = parser.items('default') if parser.has_section('default') else []\n",
    "    env = {k.upper(): v for k, v in [*cfg_items, *cred_items]}\n",
    "    with contextlib.suppress(KeyError):\n",
    "        env['AWS_REGION'] = env.pop('REGION')\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdec901f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- User‑adjustable knobs ---------------------------------------------------\n",
    "N_WORKERS = 7          # e.g. 15\n",
    "INSTANCE_TYPE = 'r5.large'  # e.g. 'c6i.xlarge'\n",
    "USE_SPOT = False       # True to save cost if interruption OK\n",
    "\n",
    "# Reinforcement Learning\n",
    "ALGORITHM = 'PPO'      # 'PPO' or 'DQN'\n",
    "TIMESTEPS = 150_000\n",
    "MAX_ITERATIONS = 15\n",
    "\n",
    "# S3\n",
    "USE_S3 = False\n",
    "S3_BUCKET = 'my‑sim‑bucket'\n",
    "S3_PREFIX = 'sim‑rl'\n",
    "\n",
    "OUTPUT_DIR = pathlib.Path('output').resolve()\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3843cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_vars = get_aws_credentials()\n",
    "env_vars['EXTRA_PIP_PACKAGES'] = 's3fs'\n",
    "\n",
    "py_tag = '-py' + re.findall(r'\\d\\.\\d+', python_version())[0]\n",
    "dask_tag = f'daskdev/dask:{dask.__version__}{py_tag}'\n",
    "\n",
    "cluster = EC2Cluster(\n",
    "    instance_type=INSTANCE_TYPE,\n",
    "    n_workers=N_WORKERS,\n",
    "    docker_image=dask_tag,\n",
    "    env_vars=env_vars,\n",
    "    security=False,\n",
    "    spot=USE_SPOT,\n",
    ")\n",
    "client = Client(cluster)\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8b7075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simulate(output_path):\n",
    "    from simulation import run_parameter_sweep\n",
    "    import os\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    run_parameter_sweep(output_filename=output_path)\n",
    "    return output_path\n",
    "\n",
    "simulation_json = OUTPUT_DIR / 'simulation_results_sweep.json'\n",
    "future = client.submit(_simulate, simulation_json.as_posix(), pure=False)\n",
    "simulation_json = pathlib.Path(future.result())\n",
    "print(f'Simulation results saved to {simulation_json}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089d818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_S3:\n",
    "    import boto3, pathlib\n",
    "    s3 = boto3.client('s3')\n",
    "    key = f\"{S3_PREFIX}/{simulation_json.name}\"\n",
    "    s3.upload_file(simulation_json.as_posix(), S3_BUCKET, key)\n",
    "    simulation_json = f's3://{S3_BUCKET}/{key}'\n",
    "    print('Uploaded to', simulation_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292940e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforment_learning import main as rl_main\n",
    "\n",
    "rl_output = OUTPUT_DIR / 'rl_results'\n",
    "rl_output.mkdir(exist_ok=True)\n",
    "\n",
    "# Build argv so reinforment_learning.py parses CLI args as intended\n",
    "sys.argv = [\n",
    "    'reinforment_learning.py',\n",
    "    '--input', str(simulation_json),\n",
    "    '--output-dir', str(rl_output),\n",
    "    '--algorithm', ALGORITHM,\n",
    "    '--timesteps', str(TIMESTEPS),\n",
    "    '--max-iterations', str(MAX_ITERATIONS),\n",
    "]\n",
    "rl_main()\n",
    "print('RL training complete; results in', rl_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ecab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()\n",
    "print('Cluster shut down.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
